{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bddf7b2",
   "metadata": {},
   "source": [
    "# Chatbot for Singer Heavy Duty Sewing Machine\n",
    "\n",
    "This is a model trained to help you retreive information and guidelines in the Singer Heavy Duty 4423 sewing machine manual. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abcd47",
   "metadata": {},
   "source": [
    "### Setup, Imports and Configure API\n",
    "\n",
    "We start by importing the necessary libraries. `fitz` (PyMuPDF) is used for extracting text from PDF pages. We also load libraries `numpy` for numerical operations, `polars` for efficient dataframes, and `tqdm`  to provide visual feedback during processing. Thereafter setting my Google Cloud API key securely with access to Generative AI models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90f56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "import polars as pl\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0ce21",
   "metadata": {},
   "source": [
    "### Load pdf-file and Extract Text \n",
    "\n",
    "We extract raw text content from each page of the pdf using PyMuPDF. This gives us a list where each item corresponds to the text from a page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f692e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 32\n"
     ]
    }
   ],
   "source": [
    "pdf_path = Path(r\"C:\\Users\\Gebruiker\\Documents\\DS24\\Chatbot\\Singer_4423_EN.pdf\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_pages = [page.get_text() for page in doc]\n",
    "    doc.close()\n",
    "    return text_pages\n",
    "\n",
    "pages = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(f\"Total pages: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee837ce",
   "metadata": {},
   "source": [
    "### Chunk the Text using *fixed-length chunking*\n",
    "\n",
    "We split the extracted text into fixed-length chunks of 1000 words with an overlap of 200 words. This allows the model to maintain context between chunks while staying within token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a767f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 7\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\\n\".join(pages)\n",
    "\n",
    "# Fixed-length chunking\n",
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # Move forward with overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(full_text, chunk_size=1000, overlap=200)\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224da2c1",
   "metadata": {},
   "source": [
    "### Embed Chunks with GenAI\n",
    "\n",
    "We will apply the `GenAI` model to generate dense vector embeddings for each textual segment, meaning we turn words into a format that the AI can really understand and compare. We then store the chunks in a `Polars` DataFrame with unique `chunk_id`s for easy reference and downstream usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c8b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.models.embed_content(\n",
    "    model=\"text-embedding-004\", \n",
    "    contents=chunks, \n",
    "    config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
    "    ).embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"chunk_id\": list(range(len(chunks))),\n",
    "    \"text_chunk\": chunks,\n",
    "    \"embedding\": [r.values for r in result]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b81d2",
   "metadata": {},
   "source": [
    "### Semantic Search Helper Functions\n",
    "\n",
    "Implement cosine similarity and a semantic search function. The search embeds a user query and finds the top k most similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5014f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def semantic_search(query, chunks, embeddings, k=5):\n",
    "    query_embedding = client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=[query],\n",
    "        config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n",
    "    ).embeddings[0].values\n",
    "\n",
    "    similarity_scores = []\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "        similarity_scores.append((i, similarity_score))\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    return [chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6171f9",
   "metadata": {},
   "source": [
    "### Generate Prompt with Retrieved Context\n",
    "\n",
    "Create a user prompt combining the question and the retrieved context chunks for use by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0757846",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\"I will ask you a question, and I want you to answer based only on the context I provide, and no other information. \n",
    "If there isn’t enough information in the context to answer the question, say 'I don’t know.' \n",
    "Do not try to guess. Express yourself clearly and divide the answer into well-structured paragraphs.\"\"\"\n",
    "\n",
    "def generate_user_prompt(query):\n",
    "    context = \"\\n\".join(semantic_search(query, df['text_chunk'], df['embedding']))\n",
    "    user_prompt = f\"This is the question: {query}. This is the context:\\n{context}.\"\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5312a5f",
   "metadata": {},
   "source": [
    "### Generate Response\n",
    "\n",
    "Send the constructed prompt and system instructions to the GenAI language model and receive a generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650bbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_prompt, user_message, model=\"gemini-2.0-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        config=types.GenerateContentConfig(system_instruction=system_prompt),\n",
    "        contents=[user_message]\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c448d3",
   "metadata": {},
   "source": [
    "### Run a Test Query\n",
    "\n",
    "Test the entire RAG pipeline with a sample question related to chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95c60f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I make a blind hem?\n",
      "Answer:\n",
      "To make a blind hem, follow these steps based on the provided context:\n",
      "\n",
      "1.  **Select Blind Hem Stitch:**\n",
      "    *   Set the Pattern Selector Dial to the blind hem setting, indicated by \"M\" in the diagram. The machine has different settings for firm and stretch fabrics.\n",
      "\n",
      "2.  **Adjust Stitch Settings:**\n",
      "    *   Set the Stitch Length Dial within the range shown in the diagram. Blind hems typically use a longer stitch length setting.\n",
      "    *   Set the Stitch Width Dial appropriately for the fabric weight. Use a narrower stitch for lighter fabrics and a wider stitch for heavier fabrics. Test on a fabric scrap first.\n",
      "\n",
      "3.  **Prepare the Fabric:**\n",
      "    *   Turn up the hem to the desired width and press it.\n",
      "    *   Fold back the hem against the right side of the fabric, leaving the top edge of the hem extending about 7 mm (1/4 inch) to the right side of the folded fabric (as shown in Fig. 1).\n",
      "\n",
      "4.  **Sew the Hem:**\n",
      "    *   Start sewing slowly on the fold.\n",
      "    *   Ensure the needle slightly catches one or two fabric threads of the folded top edge.\n",
      "\n",
      "5.  **Finish the Hem:**\n",
      "    *   Unfold the fabric after hemming.\n",
      "    *   Press the hem.\n",
      "\n",
      "6.  **Optional: Use a Blind Hem Foot:**\n",
      "    *   For easier blind hem sewing, use a blind hem foot, available from a SINGER retailer.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I make a blind hem?\"\n",
    "user_prompt = generate_user_prompt(query)\n",
    "response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(\"Answer:\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3462ff",
   "metadata": {},
   "source": [
    "### Save and Load Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d47e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sewing machine manual embeddings\n",
    "df.write_parquet(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54440cd4",
   "metadata": {},
   "source": [
    "**Diskussion**\n",
    "\n",
    "Modellen kan användas i verkligheten precis med det syftet den skapades - att enkelt lokalisera informatinon i en manual när du sitter med en ny symaskin. Detta kan vara på personnivå, men också på företagsnivå när personal kommer till en ny masin och snabbt behöver lära sig nya inställningar.\n",
    "\n",
    "En potentiell utmaning är att den är tränad för en specifik modell. Detta är också dess styrka, men för en ny maskin modell behövs alltså en ny modell. Det kan vara en utmaning om ett företag har månag olika maskiner. \n",
    "\n",
    "En utvecklingsmölighet, något jag även provade mig på, är att inkludera bilderna från manualen i svaren. Det hade gjort responsen ännu hjälpsammare och tydligare. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
